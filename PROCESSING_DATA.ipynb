{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk #import the natural language\n",
    "from nltk.stem.snowball import FrenchStemmer \n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_file(path):\n",
    "    '''reads in raw text from a text file using the argument (path), which represents the path/to/file'''\n",
    "    f = open(path,\"r\") #open the file located at \"path\" as a file object (f) that is readonly\n",
    "    raw = f.read()#.decode('utf8') # read raw text into a variable (raw) after decoding it from utf8\n",
    "    f.close() #close the file now that it isn;t being used any longer\n",
    "    return raw\n",
    "\n",
    "def get_tokens(raw,encoding='utf8'):\n",
    "    '''get the nltk tokens from a text'''\n",
    "    tokens = nltk.word_tokenize(raw) #tokenize the raw UTF-8 text\n",
    "    return tokens\n",
    "\n",
    "def get_nltk_text(raw,encoding='utf8'):\n",
    "    '''create an nltk text using the passed argument (raw) after filtering out the commas'''\n",
    "    #turn the raw text into an nltk text object\n",
    "    no_commas = re.sub(r'[.|,|\\']',' ', raw) #filter out all the commas, periods, and appostrophes using regex\n",
    "    tokens = nltk.word_tokenize(no_commas) #generate a list of tokens from the raw text\n",
    "    text=nltk.Text(tokens,encoding) #create a nltk text from those tokens\n",
    "    return text\n",
    "\n",
    "def filter_stopwords(text,stopword_list):\n",
    "    '''normalizes the words by turning them all lowercase and then filters out the stopwords'''\n",
    "    words=[w.lower() for w in text] #normalize the words in the text, making them all lowercase\n",
    "    #filtering stopwords\n",
    "    filtered_words = [] #declare an empty list to hold our filtered words\n",
    "    for word in words: #iterate over all words from the text\n",
    "        if word not in stopword_list and word.isalpha() and len(word) > 1: #only add words that are not in the French stopwords list, are alphabetic, and are more than 1 character\n",
    "            filtered_words.append(word) #add word to filter_words list if it meets the above conditions\n",
    "    filtered_words#.sort() #sort filtered_words list\n",
    "    return filtered_words\n",
    "\n",
    "def get_stopswords(type=\"veronis\"):\n",
    "    '''returns the veronis stopwords in unicode, or if any other value is passed,\n",
    "    it returns the default nltk french stopwords'''\n",
    "    if type==\"veronis\":\n",
    "        #VERONIS STOPWORDS\n",
    "        raw_stopword_list = [\"Ap.\", \"Apr.\", \"GHz\", \"MHz\", \"USD\", \"a\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\",\"aies\", \"ait\", \"alors\", \"après\", \"as\", \"attendu\", \"au\", \"au-delà\", \"au-devant\",\"aucun\", \"aucune\", \"audit\", \"auprès\", \"auquel\", \"aura\", \"aurai\", \"auraient\",\n",
    "                             \"aurais\", \"aurait\", \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autour\", \"autre\", \"autres\", \"autrui\", \"aux\", \"auxdites\", \"auxdits\",    \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\", \"avez\",\n",
    "                             \"aviez\", \"avions\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\", \"bah\", \"banco\", \"ben\",\"bien\", \"bé\", \"c\", \"c'\", \"c'est\", \"c'était\", \"car\", \"ce\", \"ceci\", \"cela\", \"celle\",\n",
    "                             \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\",\"celui-là\", \"celà\", \"cent\", \"cents\", \"cependant\", \"certain\", \"certaine\",\n",
    "                             \"certaines\", \"certains\", \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\",\"cf.\", \"cg\", \"cgr\", \"chacun\", \"chacune\", \"chaque\", \"chez\", \"ci\", \"cinq\", \"cinquante\", \"cinquante-cinq\", \"cinquante-deux\", \"cinquante-et-un\", \n",
    "                             \"cinquante-huit\", \"cinquante-neuf\", \"cinquante-quatre\", \"cinquante-sept\", \"cinquante-six\", \"cinquante-trois\", \"cl\", \"cm\", \"cm²\", \"comme\", \"contre\",\n",
    "                             \"d\", \"d'\", \"d'après\", \"d'un\", \"d'une\", \"dans\", \"de\", \"depuis\", \"derrière\", \"des\", \"desdites\", \"desdits\", \"desquelles\", \"desquels\", \"deux\", \"devant\", \"devers\", \"dg\", \"différentes\", \"différents\", \"divers\", \"diverses\", \"dix\", \n",
    "                             \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dl\", \"dm\", \"donc\", \"dont\", \"douze\", \"du\", \"dudit\", \"duquel\", \"durant\", \"dès\", \"déjà\", \"e\", \"eh\", \"elle\", \"elles\",\n",
    "                             \"en\", \"en-dehors\", \"encore\", \"enfin\", \"entre\", \"envers\", \"es\", \"est\", \"et\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\", \"eusses\",\n",
    "                             \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fait\", \"fi\", \"flac\", \"fors\", \"furent\", \"fus\", \"fusse\", \"fussent\", \"fusses\", \"fussiez\",                             \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"g\", \"gr\", \"h\", \"ha\", \"han\", \"hein\", \"hem\", \"heu\", \"hg\", \"hl\", \"hm\", \"hm³\", \"holà\", \"hop\", \"hormis\", \"hors\", \"huit\", \n",
    "                             \"hum\", \"hé\", \"i\", \"ici\", \"il\", \"ils\", \"j\", \"j'\", \"j'ai\", \"j'avais\", \"j'étais\",   \"jamais\", \"je\", \"jusqu'\", \"jusqu'au\", \"jusqu'aux\", \"jusqu'à\", \"jusque\", \"k\",\"kg\", \"km\", \"km²\", \"l\", \"l'\", \"l'autre\", \"l'on\", \"l'un\", \"l'une\", \"la\", \"laquelle\", \"le\", \"lequel\", \"les\", \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"lez\", \"lors\", \"lorsqu'\", \"lorsque\", \"lui\", \"lès\", \"m\", \"m'\", \"ma\", \"maint\", \"mainte\", \"maintes\", \"maints\", \"mais\", \"malgré\", \"me\", \"mes\", \"mg\", \"mgr\", \"mil\", \"mille\", \"milliards\", \"millions\", \"ml\", \"mm\", \"mm²\", \"moi\", \"moins\", \"mon\", \"moyennant\", \"mt\", \"m²\", \"m³\", \"même\", \"mêmes\", \"n\", \"n'avait\", \"n'y\", \"ne\", \"neuf\", \"ni\", \"non\", \"nonante\", \"nonobstant\", \"nos\", \"notre\", \"nous\", \"nul\", \"nulle\", \"nº\", \"néanmoins\", \"o\", \"octante\", \"oh\", \"on\", \"ont\", \"onze\", \"or\", \"ou\", \"outre\", \"où\", \"p\", \"par\", \"par-delà\", \"parbleu\", \"parce\", \"parmi\", \"pas\", \"passé\", \"pendant\", \"personne\", \"peu\", \"plus\", \"plus_d'un\", \"plus_d'une\", \"plusieurs\", \"pour\", \"pourquoi\", \"pourtant\", \"pourvu\", \"près\", \"puisqu'\", \"puisque\", \"q\", \"qu\", \"qu'\", \"qu'elle\", \"qu'elles\", \"qu'il\", \"qu'ils\", \"qu'on\", \"quand\", \"quant\", \"quarante\", \"quarante-cinq\", \"quarante-deux\", \"quarante-et-un\", \"quarante-huit\", \"quarante-neuf\", \"quarante-quatre\", \"quarante-sept\", \"quarante-six\", \"quarante-trois\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatre-vingt-cinq\", \"quatre-vingt-deux\", \"quatre-vingt-dix\", \"quatre-vingt-dix-huit\", \"quatre-vingt-dix-neuf\", \"quatre-vingt-dix-sept\", \"quatre-vingt-douze\", \"quatre-vingt-huit\", \"quatre-vingt-neuf\", \"quatre-vingt-onze\", \"quatre-vingt-quatorze\", \"quatre-vingt-quatre\", \"quatre-vingt-quinze\", \"quatre-vingt-seize\", \"quatre-vingt-sept\", \"quatre-vingt-six\", \"quatre-vingt-treize\", \"quatre-vingt-trois\", \"quatre-vingt-un\", \"quatre-vingt-une\", \"quatre-vingts\", \"que\", \"quel\", \"quelle\", \"quelles\", \"quelqu'\", \"quelqu'un\", \"quelqu'une\", \"quelque\", \"quelques\", \"quelques-unes\", \"quelques-uns\", \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoiqu'\", \"quoique\", \"r\", \"revoici\", \"revoilà\", \"rien\", \"s\", \"s'\", \"sa\", \"sans\", \"sauf\", \"se\", \"seize\", \"selon\", \"sept\", \"septante\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\", \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"si\", \"sinon\", \"six\", \"soi\", \"soient\", \"sois\", \"soit\", \"soixante\", \"soixante-cinq\", \"soixante-deux\", \"soixante-dix\", \"soixante-dix-huit\", \"soixante-dix-neuf\", \"soixante-dix-sept\", \"soixante-douze\", \"soixante-et-onze\", \"soixante-et-un\", \"soixante-et-une\", \"soixante-huit\", \"soixante-neuf\", \"soixante-quatorze\", \"soixante-quatre\", \"soixante-quinze\", \"soixante-seize\", \"soixante-sept\", \"soixante-six\", \"soixante-treize\", \"soixante-trois\", \"sommes\", \"son\", \"sont\", \"sous\", \"soyez\", \"soyons\", \"suis\", \"suite\", \"sur\", \"sus\", \"t\", \"t'\", \"ta\", \"tacatac\", \"tandis\", \"te\", \"tel\", \"telle\", \"telles\", \"tels\", \"tes\", \"toi\", \"ton\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\", \"treize\", \"trente\", \"trente-cinq\", \"trente-deux\", \"trente-et-un\", \"trente-huit\", \"trente-neuf\", \"trente-quatre\", \"trente-sept\", \"trente-six\", \"trente-trois\", \"trois\", \"très\", \"tu\", \"u\", \"un\", \"une\", \"unes\", \"uns\", \"v\", \"vers\", \"via\", \"vingt\", \"vingt-cinq\", \"vingt-deux\", \"vingt-huit\", \"vingt-neuf\", \"vingt-quatre\", \"vingt-sept\", \"vingt-six\", \"vingt-trois\", \"vis-à-vis\", \"voici\", \"voilà\", \"vos\", \"votre\", \"vous\", \"w\", \"x\", \"y\", \"z\", \"zéro\", \"à\", \"ç'\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\"]\n",
    "    else:\n",
    "        #get French stopwords from the nltk kit\n",
    "        raw_stopword_list = stopwords.words('french') #create a list of all French stopwords\n",
    "    stopword_list = [word for word in raw_stopword_list] #make to decode the French stopwords as unicode objects rather than ascii\n",
    "    return stopword_list\n",
    "\n",
    "def create_csv_file(filter_word, PATH, querry):\n",
    "    liste  = filtered_word\n",
    "    final = \" \"\n",
    "    i = 0\n",
    "    lista = []\n",
    "    with open(PATH +querry+'.csv', mode='w') as employee_file:\n",
    "        employee_writer = csv.writer(employee_file, delimiter = \" \")\n",
    "\n",
    "        for i in range(0, len(liste)):\n",
    "            if ((i % 130) == 0):\n",
    "                employee_writer.writerow(lista)\n",
    "                lista  =  []\n",
    "                lista.append(liste[i])\n",
    "            else :\n",
    "                lista.append(liste[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informatique.csv\n",
      "science.csv\n",
      "technologie.csv\n",
      "hotel.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    PATH_corpus_csv  =\"/home/nchet/Documents/M2_BIG_DATA/ANNA_PAPPA/Projet_finale/corpus_csv/\"\n",
    "    PATH_Data    =\"/home/nchet/Documents/M2_BIG_DATA/ANNA_PAPPA/Projet_finale/Data/\"\n",
    "    PATH_corpus  =\"/home/nchet/Documents/M2_BIG_DATA/ANNA_PAPPA/Projet_finale/CORPUS/\"\n",
    "    col    = ['description', 'label']\n",
    "    for element in os.listdir(PATH_Data):\n",
    "        if element.endswith('.txt'):\n",
    "            name  = element.split('.')\n",
    "            #path =\"./Data/voiture.txt\"\n",
    "            raw = read_raw_file(PATH_Data+element)\n",
    "            tokens = get_tokens(raw)\n",
    "            #print( \"taille of file before feltring is \\n\", len(tokens) , \"this is a sampling \" , tokens[0:15])\n",
    "            text = get_nltk_text(raw)\n",
    "            stopword_list = get_stopswords()\n",
    "            #print(\"taille of file is\", len(stopword_list))\n",
    "            #print(stopword_list)\n",
    "            filtered_word = filter_stopwords(text,stopword_list)\n",
    "            #print(filtered_word)\n",
    "            create_csv_file(filtered_word, PATH_corpus_csv, name[0])\n",
    "            #print(\"taille of file after filtring is\", len(filtered_word)\n",
    "    for element in os.listdir(PATH_corpus_csv):\n",
    "        if element.endswith('.csv'):\n",
    "            print(element)\n",
    "            name  = element.split('.')\n",
    "            df  = pd.read_csv(PATH_corpus_csv+element, sep=';', header= None)\n",
    "            df['description'] = df[:]\n",
    "            df['label'] = name[0]\n",
    "            \n",
    "            df.rename(columns={\"0\": 'description', 'label': 'label'}, inplace=True)\n",
    "            \n",
    "            df1 = pd.DataFrame({'description': df['description'],\n",
    "                              'label' : df['label']})\n",
    "            df1.to_csv(PATH_corpus+\"Dataset.csv\", mode = 'a',columns =col, header=True, sep=\";\", index=False)\n",
    "            \n",
    "            #df.to_csv(PATH_corpus+name[0]+\".csv\", columns =col, header=True, sep=\";\", index=False)\n",
    "        else :\n",
    "            pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talle is  10 ['cour', 'cour', 'il', 'maison', 'mang', 'montagn', 'prochain', 'savoir', 'téléphon', 'voir']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def stem_words(words):\n",
    "    '''stems the word list using the French Stemmer'''\n",
    "    #stemming words\n",
    "    stemmed_words = [] #declare an empty list to hold our stemmed words\n",
    "    stemmer = FrenchStemmer() #create a stemmer object in the FrenchStemmer class\n",
    "    for word in words:\n",
    "        stemmed_word=stemmer.stem(word) #stem the word\n",
    "        stemmed_words.append(stemmed_word) #add it to our stemmed word list\n",
    "    stemmed_words.sort() #sort the stemmed_words\n",
    "    return stemmed_words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hello = ['maison', 'montagne', 'il', 'voir', 'manger', 'savoir', 'cours',  'cour', 'téléphone', 'prochaine']\n",
    "    stemmed_words = stem_words(hello)\n",
    "    print(\"talle is \", len(stemmed_words), stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pseudo': ['Prolixe', 'her'], 'mot de passe': ['dfdProlixe', 'herfdf']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_dictionnaire = {}\n",
    "mon_dictionnaire[\"pseudo\"] = [\"Prolixe\", \"her\"]\n",
    "mon_dictionnaire[\"mot de passe\"] = [\"dfdProlixe\", \"herfdf\"]\n",
    "mon_dictionnaire\n",
    "#{'mot de passe': '*', 'pseudo': 'Prolixe'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "numbers = [1,2,3,4,5,1,4,5] \n",
    "  \n",
    "# start parameter is not provided \n",
    "Sum = sum(numbers) \n",
    "print(Sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "dictionary = {\"raj\": 2, \"striver\": 3, \"vikram\": 4} \n",
    "print(dictionary.values())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aujourd'hui\",\n",
       " 'il',\n",
       " 'fait',\n",
       " 'super',\n",
       " 'beau',\n",
       " 'et',\n",
       " 'ce',\n",
       " 'soir',\n",
       " 'aussi',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"\"\" aujourd'hui il fait super beau et ce soir aussi . \"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"aujourd'hui\", 'NN'),\n",
       " ('il', 'NN'),\n",
       " ('fait', 'VBP'),\n",
       " ('super', 'JJR'),\n",
       " ('beau', 'NN'),\n",
       " ('et', 'FW')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
